{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport spacy\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-07T21:19:05.124283Z","iopub.execute_input":"2023-06-07T21:19:05.124683Z","iopub.status.idle":"2023-06-07T21:19:05.132797Z","shell.execute_reply.started":"2023-06-07T21:19:05.124652Z","shell.execute_reply":"2023-06-07T21:19:05.131493Z"},"trusted":true},"execution_count":271,"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"def load_data(file):\n    with open(file='r', encoding='utf-8') as f:\n        data = pd.read_csv(f)\n        return (data)\n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path='/kaggle/input/nlp-getting-started/test.csv'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = load_data(path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"\n\ndf = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ndf.head(1)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-07T21:19:05.141828Z","iopub.execute_input":"2023-06-07T21:19:05.142834Z","iopub.status.idle":"2023-06-07T21:19:05.185037Z","shell.execute_reply.started":"2023-06-07T21:19:05.142702Z","shell.execute_reply":"2023-06-07T21:19:05.184067Z"},"trusted":true},"execution_count":272,"outputs":[{"execution_count":272,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n\n   target  \n0       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-06-07T21:19:05.186750Z","iopub.execute_input":"2023-06-07T21:19:05.187299Z","iopub.status.idle":"2023-06-07T21:19:05.204948Z","shell.execute_reply.started":"2023-06-07T21:19:05.187264Z","shell.execute_reply":"2023-06-07T21:19:05.203629Z"},"trusted":true},"execution_count":273,"outputs":[{"execution_count":273,"output_type":"execute_result","data":{"text/plain":"id             0\nkeyword       61\nlocation    2533\ntext           0\ntarget         0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-07T21:19:05.206757Z","iopub.execute_input":"2023-06-07T21:19:05.207204Z","iopub.status.idle":"2023-06-07T21:19:05.216909Z","shell.execute_reply.started":"2023-06-07T21:19:05.207167Z","shell.execute_reply":"2023-06-07T21:19:05.215614Z"},"trusted":true},"execution_count":274,"outputs":[{"execution_count":274,"output_type":"execute_result","data":{"text/plain":"(7613, 5)"},"metadata":{}}]},{"cell_type":"code","source":"df = df[['text','target']]","metadata":{"execution":{"iopub.status.busy":"2023-06-07T21:19:05.220387Z","iopub.execute_input":"2023-06-07T21:19:05.220840Z","iopub.status.idle":"2023-06-07T21:19:05.230175Z","shell.execute_reply.started":"2023-06-07T21:19:05.220803Z","shell.execute_reply":"2023-06-07T21:19:05.228973Z"},"trusted":true},"execution_count":275,"outputs":[]},{"cell_type":"code","source":"df.iloc[0][\"text\"]","metadata":{"execution":{"iopub.status.busy":"2023-06-07T21:19:05.232294Z","iopub.execute_input":"2023-06-07T21:19:05.232707Z","iopub.status.idle":"2023-06-07T21:19:05.246017Z","shell.execute_reply.started":"2023-06-07T21:19:05.232672Z","shell.execute_reply":"2023-06-07T21:19:05.244733Z"},"trusted":true},"execution_count":276,"outputs":[{"execution_count":276,"output_type":"execute_result","data":{"text/plain":"'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'"},"metadata":{}}]},{"cell_type":"code","source":"df.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T21:19:05.247232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp = spacy.load('en_core_web_sm')\n#from spacy.lang.en import English\n#nlp = English()\n#nlp = spacy.blank('en')\n#nlp.pipe_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we created a simple Text cat we are adding that to spacy :D \ntextcat = nlp.add_pipe('textcat',last=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp.pipe_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we created a simple Text cat we are adding that to spacy :D \ntextcat = nlp.create_pipe(\"textcat\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp.pipe_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding the labels to textcat\ntextcat.add_label(\"POSITIVE\")\ntextcat.add_label(\"NEGATIVE\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"textcat.labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting review text to tuple \ndf['tuples'] = df.apply(lambda row: (row['text'],row['target']), axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is how data looks like \ndf[\"tuples\"][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting tuple to List \ntrain = df['tuples'].tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train[4])\n#print(len(train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts, labels = zip(*train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#what we did is bascially created Flags if its Positive aka True and negative we gave False\ncats = []\nfor y in labels:\n    if(bool(y)):\n        cats.append({\"POSITIVE\": True, \"NEGATIVE\":False})\n    else:\n        cats.append({\"POSITIVE\": False, \"NEGATIVE\":True})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TrainX = texts\nTrainY = cats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(TrainX)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(TrainY)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#training data has to be in this format\ntrain_data = list(zip(TrainX,[{'cats': cats} for cats in TrainY]))\ntrain_data[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model","metadata":{}},{"cell_type":"code","source":"n_iter=20","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from thinc.api import Adam\n\noptimizer = Adam(\n    learn_rate=0.001,\n    beta1=0.9,\n    beta2=0.999,\n    eps=1e-08,\n    L2=1e-6,\n    grad_clip=1.0,\n    use_averages=True,\n    L2_is_weight_decay=True\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from spacy.util import minibatch, compounding\nfrom spacy.training.example import Example\n# Construction from subclass\nfrom spacy.lang.en import English\nnlp = English()\n\n# we created a simple Text cat we are adding that to spacy :D \ntextcat = nlp.add_pipe('textcat',last=True)\n\n# Adding the labels to textcat\ntextcat.add_label(\"POSITIVE\")\ntextcat.add_label(\"NEGATIVE\")\n\n# Converting review text to tuple \ndf['tuples'] = df.apply(lambda row: (row['text'],row['target']), axis=1)\n\n# Converting tuple to List \ntrain = df['tuples'].tolist()\ntexts, labels = zip(*train)\n\n#what we did is bascially created Flags if its Positive aka True and negative we gave False\ncats = []\nfor y in labels:\n    if(bool(y)):\n        cats.append({\"POSITIVE\": True, \"NEGATIVE\":False})\n    else:\n        cats.append({\"POSITIVE\": False, \"NEGATIVE\":True})\n        \nTrainX = texts\nTrainY = cats\n\n#training data has to be in this format\ntrain_data = list(zip(TrainX,[{'cats': cats} for cats in TrainY]))\n\n# Disabling other components\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\nwith nlp.disable_pipes(*other_pipes):  # only train textcat\n    #optimizer = nlp.begin_training()\n    optimizer = optimizer\n    print(\"Training the model...\")\n    \n    # Performing training\n    for i in range(len(train_data)):\n        print(\"Epoch : {} \".format(i))\n        losses = {}\n        batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n        for batch in batches:\n\n            texts, annotations = zip(*batch)\n                \n            example = []\n                # Update the model with iterating each text\n            for i in range(len(texts)):\n                doc = nlp.make_doc(texts[i])\n                example.append(Example.from_dict(doc, annotations[i]))\n                \n                # Update the model\n            nlp.update(example, drop=0.5, losses=losses)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp.to_disk(\"sentiment\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp = spacy.load(\"sentiment\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing the model\ntest_text = \"I had such high hopes for this dress and really crappy worst product hate it wporst bad \"\ndoc=nlp(test_text)\ndoc.cats ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing the model\ndoc=nlp(texts[1])\ndoc.cats ","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}